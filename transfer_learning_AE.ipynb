{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning Project Work  \n<a href=\"https://www.kaggle.com/pirimidin/deep-learning-hw-milestone-1\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>","metadata":{}},{"cell_type":"markdown","source":"### Transfer Learning solution\n\nThis solution of our project work is based around transfer learning, to find out more please read the documentation.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport torchvision\nimport math\nfrom skimage.metrics import structural_similarity\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-12-11T19:45:22.090166Z","iopub.status.idle":"2022-12-11T19:45:22.091169Z","shell.execute_reply.started":"2022-12-11T19:45:22.090881Z","shell.execute_reply":"2022-12-11T19:45:22.090909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading data from the MVTecAD database - Carpet and Hazelnut categories were used\n\n!wget https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\n!wget https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz\n!tar -xf hazelnut.tar.xz \n!tar -xf carpet.tar.xz\n!pip install piqa","metadata":{"execution":{"iopub.status.busy":"2022-12-11T16:38:02.788636Z","iopub.execute_input":"2022-12-11T16:38:02.789062Z","iopub.status.idle":"2022-12-11T16:39:47.192223Z","shell.execute_reply.started":"2022-12-11T16:38:02.789010Z","shell.execute_reply":"2022-12-11T16:39:47.190895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_image_visualization():\n\n  image_paths = ['./hazelnut/test/crack/000.png','./hazelnut/test/cut/006.png',\n                 './hazelnut//test/hole/013.png','./carpet/test/metal_contamination/004.png',\n                 './carpet/test/color/005.png', './carpet/test/thread/008.png']\n  mask_paths = ['./hazelnut/ground_truth/crack/000_mask.png','./hazelnut/ground_truth/cut/006_mask.png',\n                './hazelnut/ground_truth/hole/013_mask.png','./carpet/ground_truth/metal_contamination/004_mask.png',\n                './carpet/ground_truth/color/005_mask.png', './carpet/ground_truth/thread/008_mask.png']\n\n  for i in range(0,len(image_paths)):\n\n    msk = cv2.imread(str(mask_paths[i]))\n    img = cv2.imread(str(image_paths[i]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    n_msk = mask_color_magenta(msk)\n    plot_images(img, n_msk)\n    \n    \n    \ndef plot_images(img, n_msk):\n\n  plt.figure(figsize = (20,20))\n\n  plt.subplot(131)\n  plt.imshow(img)\n  plt.xticks([]), plt.yticks([])\n\n  plt.subplot(132)\n  plt.imshow(n_msk)\n  plt.xticks([]), plt.yticks([])\n\n  plt.subplot(133)\n  plt.imshow(img)\n  plt.imshow(n_msk, alpha = 0.3)\n  plt.xticks([]), plt.yticks([])\n  \n  plt.show()\n\n\n\ndef mask_color_magenta(img):\n  # Modifies the original mask image - makes background transparent + sets the \n  # color of the segmented parts to magenta\n\n  # separate R,G,B color channels and create a mask for the white pixels of the image\n  mask = (img[:,:,0] == 255) & (img[:,:,1] == 255) & (img[:,:,2] == 255)\n  # set the new color for white coloured pixels\n  img[:,:,:3][mask] = [255,0,255] \n  # convert image to grayscale and \"create a dimension for opacity\"\n  tmp = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n  _, alpha = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY)\n  # split image by color channels and append 4th 'opacity' dimension\n  b, g, r = cv2.split(img)\n  rgba = [b, g, r, alpha]\n  n_img = cv2.merge(rgba, 4)\n\n  return n_img\n\ntest_image_visualization()","metadata":{"execution":{"iopub.status.busy":"2022-12-10T08:42:07.734232Z","iopub.execute_input":"2022-12-10T08:42:07.734702Z","iopub.status.idle":"2022-12-10T08:42:13.184284Z","shell.execute_reply.started":"2022-12-10T08:42:07.734661Z","shell.execute_reply":"2022-12-10T08:42:13.183408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model section\n\nOur model is a pretty simple autoencoder architecture with extra added convolutional layers. This convolutional section helps us better deal with image data. We have to keep the autoencoder architecture in mind, when designing the convolutional section, because in a way that the encoder's reverse is the decoder, this way our convolutional section has to have a deconvolutional section as well, which basically means using transposed convolutional layers.","metadata":{}},{"cell_type":"code","source":"\nimport torch.nn as nn\n\n# First, let's load a pre-trained model and freeze all of the layers\nmodel = torchvision.models.vgg19(pretrained=True)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Next, let's define our own autoencoder model, using the pre-trained model as the encoder\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        # Define the encoder part of the model\n        self.encoder = nn.Sequential(\n            # Use the first few layers of VGG-19 as the encoder\n            *list(model.features.children())[:15]\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.Dropout(0.2),\n            nn.Sigmoid()\n        )\n        \n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:34:32.486126Z","iopub.execute_input":"2022-12-11T21:34:32.486500Z","iopub.status.idle":"2022-12-11T21:34:34.314053Z","shell.execute_reply.started":"2022-12-11T21:34:32.486470Z","shell.execute_reply":"2022-12-11T21:34:34.312980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f'Selected device: {device}')\n\n### Define SSIM loss function\nfrom piqa import SSIM\n\nclass SSIMLoss(SSIM):\n    def forward(self, x, y):\n        return 1. - super().forward(x, y)\n\n### Define the loss function\nloss_fn = torch.nn.MSELoss()\nssim_loss = SSIMLoss()\n\n### Set the learning rate\nlr= 0.008\n\n### Set the random seed for reproducible results\ntorch.manual_seed(0)\n\n### Initialize the autoencoder\nautoencoder = Autoencoder()\n\n### Define an optimizer \noptim = torch.optim.Adam(autoencoder.parameters(), lr=lr, weight_decay=1e-05)\n### Define the scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)\n\n# Move the autoencoder to the selected device\nautoencoder.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:34:34.596815Z","iopub.execute_input":"2022-12-11T21:34:34.597345Z","iopub.status.idle":"2022-12-11T21:34:34.627726Z","shell.execute_reply.started":"2022-12-11T21:34:34.597304Z","shell.execute_reply":"2022-12-11T21:34:34.626886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Dataloader creation","metadata":{}},{"cell_type":"code","source":"# Define a transform function that is going to be used on our images\ntransform = torchvision.transforms.Compose([torchvision.transforms.Resize(32),\n                                torchvision.transforms.ToTensor(),\n                                #torchvision.transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n                               ])\n# Load in the datasets\nhazelnut_train_dataset = torchvision.datasets.ImageFolder(\"./hazelnut/train/\",transform=transform)\nhazelnut_test_dataset = torchvision.datasets.ImageFolder(\"./hazelnut/test/\",transform=transform)\n\ncarpet_train_dataset = torchvision.datasets.ImageFolder(\"./carpet/train/\",transform=transform)\ncarpet_test_dataset = torchvision.datasets.ImageFolder(\"./carpet/test/\",transform=transform)\n\n# Split the data into training and validation\nhazelnut_test_length = int(len(hazelnut_test_dataset) * 0.8)\nhazelnut_val_length = len(hazelnut_test_dataset) - hazelnut_test_length\n\ncarpet_test_length = int(len(carpet_test_dataset) * 0.8)\ncarpet_val_length = len(carpet_test_dataset) - carpet_test_length\n\nhazelnut_test_dataset, hazelnut_val_dataset = torch.utils.data.random_split(hazelnut_test_dataset,[hazelnut_test_length,hazelnut_val_length],generator=torch.Generator().manual_seed(42))\ncarpet_test_dataset, carpet_val_dataset = torch.utils.data.random_split(carpet_test_dataset,[carpet_test_length,carpet_val_length],generator=torch.Generator().manual_seed(42))\n\n# Define our DataLoader objects\nbatch_size = 32\nhazelnut_train_loader  = torch.utils.data.DataLoader(hazelnut_train_dataset,batch_size=batch_size,shuffle=True)\nhazelnut_test_loader  = torch.utils.data.DataLoader(hazelnut_test_dataset,batch_size=batch_size,shuffle=True)\nhazelnut_val_loader  = torch.utils.data.DataLoader(hazelnut_val_dataset,batch_size=batch_size,shuffle=True)\ncarpet_train_loader  = torch.utils.data.DataLoader(carpet_train_dataset,batch_size=batch_size,shuffle=True)\ncarpet_test_loader  = torch.utils.data.DataLoader(carpet_test_dataset,batch_size=batch_size,shuffle=True)\ncarpet_val_loader  = torch.utils.data.DataLoader(carpet_val_dataset,batch_size=batch_size,shuffle=True)\n\n# DataLoader for visualization\nhazelnut_vis_test_loader = torch.utils.data.DataLoader(hazelnut_test_dataset,batch_size=len(hazelnut_test_dataset))\ncarpet_vis_test_loader = torch.utils.data.DataLoader(carpet_test_dataset,batch_size=len(hazelnut_test_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:15:00.361612Z","iopub.execute_input":"2022-12-11T21:15:00.362025Z","iopub.status.idle":"2022-12-11T21:15:00.380609Z","shell.execute_reply.started":"2022-12-11T21:15:00.361993Z","shell.execute_reply":"2022-12-11T21:15:00.379307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train, Test, Evaluation, and Visualization functions","metadata":{}},{"cell_type":"code","source":"def train_epoch(autoencoder, device, dataloader, loss_fn, scheduler,optimizer):\n    # Set train mode for both the encoder and the decoder\n    autoencoder.train()\n    ssim_loss.to('cuda:0')\n    train_loss = []\n    train_loss_ssim = []\n    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n        # Move tensor to the proper device\n        image_batch = image_batch.to(device)\n        # Encode data\n        decoded_data = autoencoder(image_batch)\n        # Evaluate loss\n        loss = loss_fn(decoded_data, image_batch)\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Forward propogation\n        optimizer.step()\n        scheduler.step()\n        \n        # Print batch loss\n        print('\\t partial train loss (single batch): %f' % (loss.data))\n        train_loss.append(loss.detach().cpu().numpy())\n        \n\n    return np.mean(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-12-11T20:58:27.942261Z","iopub.execute_input":"2022-12-11T20:58:27.942631Z","iopub.status.idle":"2022-12-11T20:58:27.949973Z","shell.execute_reply.started":"2022-12-11T20:58:27.942600Z","shell.execute_reply":"2022-12-11T20:58:27.948983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_epoch(autoencoder, device, dataloader, loss_fn):\n    # Set evaluation mode for encoder and decoder\n    autoencoder.eval()\n    \n    with torch.no_grad(): # No need to track the gradients\n        # Define the lists to store the outputs for each batch\n        conc_out = []\n        conc_label = []\n        for image_batch, _ in dataloader:\n            # Move tensor to the proper device\n            image_batch = image_batch.to(device)\n            # Decode data\n            decoded_data = autoencoder(image_batch)\n            # Append the network output and the original image to the lists\n            conc_out.append(decoded_data.cpu())\n            conc_label.append(image_batch.cpu())\n        # Create a single tensor with all the values in the lists\n        conc_out = torch.cat(conc_out)\n        conc_label = torch.cat(conc_label) \n        # Evaluate global loss\n        val_loss = loss_fn(conc_out, conc_label)\n        \n        \n    return val_loss.data","metadata":{"execution":{"iopub.status.busy":"2022-12-11T20:58:30.384795Z","iopub.execute_input":"2022-12-11T20:58:30.385506Z","iopub.status.idle":"2022-12-11T20:58:30.392457Z","shell.execute_reply.started":"2022-12-11T20:58:30.385470Z","shell.execute_reply":"2022-12-11T20:58:30.391511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ae_outputs(autoencoder,vis_test_loader, n=5): # n stands for the number of classes\n    plt.figure(figsize=(16,4.5))\n    # Get the test images and labels from our loader\n    images ,targets = next(iter(vis_test_loader))\n    # Group them into a dictionary with keys as possible labels\n    t_idx = {i:np.where(targets==i)[0] for i in range(n)}\n    # Plot for each class\n    for i in range(n):\n      ax = plt.subplot(2,n,i+1)\n      # Get our image from the test dataset\n      img = images[t_idx[i]][0].unsqueeze(0).to(device)\n      autoencoder.eval()\n      # Put the test image through our autoencoder\n      with torch.no_grad():\n         rec_img  = autoencoder(img)\n      plt.imshow(img.T.cpu().squeeze().numpy(), cmap='gist_gray')\n      ax.get_xaxis().set_visible(False)\n      ax.get_yaxis().set_visible(False)  \n      if i == n//2:\n        ax.set_title('Original images')\n      ax = plt.subplot(2, n, i + 1 + n)\n      plt.imshow(rec_img.T.cpu().squeeze().numpy(), cmap='gist_gray')  \n      ax.get_xaxis().set_visible(False)\n      ax.get_yaxis().set_visible(False)  \n      if i == n//2:\n         ax.set_title('Reconstructed images')\n    plt.show()   ","metadata":{"execution":{"iopub.status.busy":"2022-12-11T20:58:32.692571Z","iopub.execute_input":"2022-12-11T20:58:32.693275Z","iopub.status.idle":"2022-12-11T20:58:32.704113Z","shell.execute_reply.started":"2022-12-11T20:58:32.693230Z","shell.execute_reply":"2022-12-11T20:58:32.703264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_ssim(img1, img2):\n    img1 = img1.cpu().numpy()\n    img2 = img2.cpu().numpy()\n    # Convert the images to grayscale if they are not already\n    if img1.ndim == 3:\n        img1 = np.mean(img1, axis=2)\n    if img2.ndim == 3:\n        img2 = np.mean(img2, axis=2)\n\n    # Compute the SSIM between the two images\n    ssim = structural_similarity(img1, img2, data_range=img1.max() - img1.min(), multichannel=False)\n\n    return ssim\n\n# Define a function to compute the SSIM between two batches of images\ndef compute_batch_ssim(batch1, batch2):\n    # Check that the batches have the same size\n    if batch1.shape != batch2.shape:\n        raise ValueError('Batch size does not match')\n\n    # Compute the SSIM for each pair of images in the batch\n    ssim_values = []\n    for img1, img2 in zip(batch1, batch2):\n        ssim = compute_ssim(img1, img2)\n        ssim_values.append(ssim)\n\n    # Return the average SSIM value\n    return np.mean(ssim_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_threshold(autoencoder, dataloader, fpr = 0.05):\n    # Compute the error threshold on the trained model with training dataset.\n    # The threshold is the q-th percentile of the output pixels' squared errors\n    autoencoder.eval()\n    \n    errors = []\n    ssim_errors = []\n    with torch.no_grad():\n        for image_batch, _ in dataloader:\n            image_batch = image_batch.to(device)\n\n            rec_img_batch = autoencoder(image_batch)\n\n            error_batch = torch.sum((rec_img_batch - image_batch)**2, axis=1).cpu().numpy()\n            ssim_error_batch = compute_batch_ssim(rec_img_batch, image_batch)\n            errors.extend(error_batch)\n            ssim_errors.extend(ssim_error_batch)\n\n    threshold = np.percentile(errors, 100-fpr)\n    ssim_threshold = np.percentile(ssim_errors, 100-fpr)\n    return threshold, ssim_threshold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(autoencoder, dataloader, threshold):\n    # Calculates errors on the given dataset and compares it to the computed threshold (error > t.hold -> anomaly)\n    # Returns a list of booleans, where each item is true if the model detected an anomaly.\n    autoencoder.eval()\n    predictions = []\n    ssim_predictions = []\n    with torch.no_grad():\n        for image_batch, _ in dataloader:\n            image_batch = image_batch.to(device)\n            \n            rec_img_batch = autoencoder(image_batch)\n            \n            error_batch = torch.sum((rec_img_batch - image_batch)**2, axis=1).T.cpu().numpy()\n            ssim_error_batch = compute_batch_ssim(rec_img_batch, image_batch)\n            ssim_pred_batch = np.any(ssim_error_batch > threshold, axis=(0,1))\n            pred_batch = np.any(error_batch > threshold, axis=(0,1))\n            \n            predictions.extend(pred_batch)\n            ssim_predictions.extend(ssim_pred_batch)\n            \n    return predictions, ssim_pred_batch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(autoencoder, dataloader, threshold):\n    # Calculates errors on the given (test) dataset and compares it to the computed threshold (error > t.hold -> anomaly)\n    # Returns a list of booleans, where each item is true if the prediction is equal to the actual value\n    autoencoder.eval()\n   \n    predictions = []\n    ssim_predictions = []\n    labels = []\n    ssim_labels = []\n    with torch.no_grad():\n        for image_batch, label_batch in dataloader:\n            image_batch = image_batch.to(device)\n            \n            rec_img_batch = autoencoder(image_batch)\n            \n            error_batch = torch.sum((rec_img_batch - image_batch)**2, axis=1).cpu().numpy()\n            ssim_error_batch = compute_batch_ssim(rec_img_batch, image_batch)\n            ssim_pred_batch = np.any(ssim_error_batch > threshold, axis=(0,1))\n            pred_batch = np.any(error_batch > threshold, axis=(1,2))\n            \n            predictions.extend(pred_batch)\n            ssim_predictions.extend(ssim_predictions)\n            \n            labels.extend(label_batch.cpu().numpy() != 2)\n            \n    return predictions,ssim_predictions, labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def experimental_evaluation(autoencoder, dataset, threshold,ssim_threshold, n=10):\n    # Evaluation wiyh n samples and visualization \n    autoencoder.eval()\n    for i, (img, label) in enumerate(dataset):\n        if i == n - 1:\n            break\n        \n        img = img.unsqueeze(0).to(device)\n        with torch.no_grad():\n            rec_img = autoencoder(img)\n            print(rec_img.shape)\n        error = torch.sum((rec_img - img)**2, axis=0).T.cpu().squeeze().numpy()\n        ssim_error = compute_ssim(rec_img,img)\n        \n        prediction = np.any(error > threshold)\n        ssim_pred = np.any(ssim_error > threshold)\n        mask = np.where(error > threshold, 255, 0)\n        ssim_mask = np.where(ssim_error > threshold,255,0)\n        \n        _, axarr = plt.subplots(2,2, figsize=(15,15))\n        axarr[0][0].imshow(img.T.cpu().squeeze().numpy(), cmap='gist_gray')\n        axarr[0][1].imshow(img.T.cpu().squeeze().numpy(), cmap='gist_gray')\n        axarr[0][1].imshow(mask, cmap='jet', alpha=0.5, interpolation='none')\n        axarr[1][0].imshow(img.T.cpu().squeeze().numpy(), cmap='gist_gray')\n        axarr[1][1].imshow(img.T.cpu().squeeze().numpy(), cmap='gist_gray')\n        axarr[1][1].imshow(ssim_mask, cmap='jet', alpha=0.5, interpolation='none')\n        plt.show()\n\n        print(f\"Threshold: {threshold}, SSIM_Threshold: {ssim_threshold} MaxValue: {np.max(error)} , SSIM MaxValue: {np.max(ssim_error)}\")\n        print(f\"label: {label}, anomaly detected: {prediction}, ssim prediction: {ssim_pred}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-11T16:52:37.495751Z","iopub.execute_input":"2022-12-11T16:52:37.496166Z","iopub.status.idle":"2022-12-11T16:52:37.510366Z","shell.execute_reply.started":"2022-12-11T16:52:37.496136Z","shell.execute_reply":"2022-12-11T16:52:37.509481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training loops","metadata":{}},{"cell_type":"markdown","source":"## Training on the hazelnut dataset","metadata":{}},{"cell_type":"code","source":"# Define the number of epochs to use for training\nnum_epochs = 80\n# Unified loss dictionary\nh_diz_loss = {'train_loss':[],'val_loss':[]}\nfor epoch in range(num_epochs):\n    \n   # Calculate training loss\n   h_train_loss = train_epoch(autoencoder,device,\n   hazelnut_train_loader,loss_fn,scheduler,optim)\n    \n   # Calculate validation loss\n   h_val_loss = test_epoch(autoencoder,device,hazelnut_test_loader,loss_fn)\n   print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {} '.format(epoch + 1, num_epochs,h_train_loss,h_val_loss))\n   h_diz_loss['train_loss'].append(h_train_loss)\n   h_diz_loss['val_loss'].append(h_val_loss)\n   plot_ae_outputs(autoencoder,hazelnut_vis_test_loader,n=5)","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:34:49.128120Z","iopub.execute_input":"2022-12-11T21:34:49.128495Z","iopub.status.idle":"2022-12-11T21:46:17.941330Z","shell.execute_reply.started":"2022-12-11T21:34:49.128465Z","shell.execute_reply":"2022-12-11T21:46:17.939440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hazelnut losses","metadata":{}},{"cell_type":"code","source":"# Plot losses for hazelnuts\nplt.figure(figsize=(10,8))\nplt.semilogy(h_diz_loss['train_loss'], label='Train')\nplt.semilogy(h_diz_loss['val_loss'], label='Valid')\nplt.xlabel('Epoch')\nplt.ylabel('Average Loss')\n#plt.grid()\nplt.legend()\nplt.title('Loss on hazelnuts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:46:23.885703Z","iopub.execute_input":"2022-12-11T21:46:23.886119Z","iopub.status.idle":"2022-12-11T21:46:24.291344Z","shell.execute_reply.started":"2022-12-11T21:46:23.886089Z","shell.execute_reply":"2022-12-11T21:46:24.290308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hazelnut experimental evaluation","metadata":{"execution":{"iopub.status.busy":"2022-11-20T15:05:29.709531Z","iopub.execute_input":"2022-11-20T15:05:29.710065Z","iopub.status.idle":"2022-11-20T15:05:29.736306Z","shell.execute_reply.started":"2022-11-20T15:05:29.709957Z","shell.execute_reply":"2022-11-20T15:05:29.735038Z"}}},{"cell_type":"code","source":"threshold, ssim_threshold = compute_threshold(autoencoder, hazelnut_train_dataset)\nprint(f\"Treshhold: {threshold}\")\nprint(f\"SSIM Treshhold: {ssim_threshold}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:46:27.757967Z","iopub.execute_input":"2022-12-11T21:46:27.758340Z","iopub.status.idle":"2022-12-11T21:46:47.194417Z","shell.execute_reply.started":"2022-12-11T21:46:27.758311Z","shell.execute_reply":"2022-12-11T21:46:47.192093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the accuracy of the anomaly prediction on the test dataset, and visualize some samples","metadata":{}},{"cell_type":"code","source":"results, ssim_results = evaluation(autoencoder, hazelnut_test_dataset, threshold, ssim_threshold)\naccuracy = sum(results) / len(results)\nssim_acc = sum(ssim_results) / len(ssim_results)\nprint(f\"ACC: {accuracy}, SSIM ACC: {ssim_acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:47:03.591264Z","iopub.execute_input":"2022-12-11T21:47:03.591667Z","iopub.status.idle":"2022-12-11T21:47:07.831736Z","shell.execute_reply.started":"2022-12-11T21:47:03.591637Z","shell.execute_reply":"2022-12-11T21:47:07.830612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experimental_evaluation(autoencoder, hazelnut_test_dataset, threshold, ssim_threshold)","metadata":{"execution":{"iopub.status.busy":"2022-12-11T21:47:12.458353Z","iopub.execute_input":"2022-12-11T21:47:12.459056Z","iopub.status.idle":"2022-12-11T21:47:18.275261Z","shell.execute_reply.started":"2022-12-11T21:47:12.459020Z","shell.execute_reply":"2022-12-11T21:47:18.274084Z"},"trusted":true},"execution_count":null,"outputs":[]}]}